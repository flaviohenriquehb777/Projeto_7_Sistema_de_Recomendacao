{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Avançada de Explicabilidade com SHAP\n",
    "\n",
    "Este notebook apresenta técnicas avançadas de explicabilidade para o modelo de recomendação utilizando SHAP (SHapley Additive exPlanations).\n",
    "\n",
    "Vamos explorar:\n",
    "1. Visualizações avançadas de SHAP\n",
    "2. Análise de dependência de features\n",
    "3. Explicabilidade por grupos de clientes\n",
    "4. Comparação de explicações entre diferentes modelos\n",
    "5. Geração de relatórios personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importações necessárias\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "from io import BytesIO\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Adicionar diretório raiz ao path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Importar funções personalizadas\n",
    "from src.config.model_utils import create_model_with_regularization\n",
    "from src.mlflow_config import setup_mlflow, log_model_metrics, load_registered_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configurar MLflow\n",
    "experiment_id = setup_mlflow(experiment_name=\"shap_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Carregar dados\n",
    "dados = pd.read_parquet('../dados/dados_tratados.parquet')\n",
    "print(f\"Dados carregados: {dados.shape}\")\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preparar dados para o modelo\n",
    "def preparar_dados(dados):\n",
    "    # Codificar variáveis categóricas\n",
    "    customer_encoder = LabelEncoder()\n",
    "    product_encoder = LabelEncoder()\n",
    "    category_encoder = LabelEncoder()\n",
    "    subcategory_encoder = LabelEncoder()\n",
    "    \n",
    "    # Aplicar encoding\n",
    "    customer_ids = customer_encoder.fit_transform(dados['customer_id'])\n",
    "    product_ids = product_encoder.fit_transform(dados['product_id'])\n",
    "    category_ids = category_encoder.fit_transform(dados['category'])\n",
    "    subcategory_ids = subcategory_encoder.fit_transform(dados['subcategory'])\n",
    "    \n",
    "    # Target\n",
    "    sales = dados['sales'].values\n",
    "    \n",
    "    # Dimensões para embeddings\n",
    "    num_customers = len(customer_encoder.classes_)\n",
    "    num_products = len(product_encoder.classes_)\n",
    "    num_categories = len(category_encoder.classes_)\n",
    "    num_subcategories = len(subcategory_encoder.classes_)\n",
    "    \n",
    "    print(f\"Número de clientes únicos: {num_customers}\")\n",
    "    print(f\"Número de produtos únicos: {num_products}\")\n",
    "    print(f\"Número de categorias únicas: {num_categories}\")\n",
    "    print(f\"Número de subcategorias únicas: {num_subcategories}\")\n",
    "    \n",
    "    # Dividir em treino e teste\n",
    "    X = (customer_ids, product_ids, category_ids, subcategory_ids)\n",
    "    y = sales\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        list(zip(customer_ids, product_ids, category_ids, subcategory_ids)),\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Descompactar X_train e X_test\n",
    "    customer_train, product_train, category_train, subcategory_train = zip(*X_train)\n",
    "    customer_test, product_test, category_test, subcategory_test = zip(*X_test)\n",
    "    \n",
    "    # Converter para arrays numpy\n",
    "    customer_train = np.array(customer_train)\n",
    "    product_train = np.array(product_train)\n",
    "    category_train = np.array(category_train)\n",
    "    subcategory_train = np.array(subcategory_train)\n",
    "    \n",
    "    customer_test = np.array(customer_test)\n",
    "    product_test = np.array(product_test)\n",
    "    category_test = np.array(category_test)\n",
    "    subcategory_test = np.array(subcategory_test)\n",
    "    \n",
    "    # Retornar dados preparados\n",
    "    return {\n",
    "        'X_train': [customer_train, product_train, category_train, subcategory_train],\n",
    "        'y_train': y_train,\n",
    "        'X_test': [customer_test, product_test, category_test, subcategory_test],\n",
    "        'y_test': y_test,\n",
    "        'encoders': {\n",
    "            'customer': customer_encoder,\n",
    "            'product': product_encoder,\n",
    "            'category': category_encoder,\n",
    "            'subcategory': subcategory_encoder\n",
    "        },\n",
    "        'dims': {\n",
    "            'num_customers': num_customers,\n",
    "            'num_products': num_products,\n",
    "            'num_categories': num_categories,\n",
    "            'num_subcategories': num_subcategories\n",
    "        },\n",
    "        'dados_originais': dados\n",
    "    }\n",
    "\n",
    "# Preparar dados\n",
    "dados_prep = preparar_dados(dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregar ou Treinar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tentar carregar modelo registrado\n",
    "model = load_registered_model(model_name=\"recommendation_model\", stage=\"Production\")\n",
    "\n",
    "# Se não encontrar modelo registrado, treinar um novo\n",
    "if model is None:\n",
    "    print(\"Modelo registrado não encontrado. Treinando novo modelo...\")\n",
    "    \n",
    "    # Criar modelo com regularização\n",
    "    model = create_model_with_regularization(\n",
    "        num_customers=dados_prep['dims']['num_customers'],\n",
    "        num_products=dados_prep['dims']['num_products'],\n",
    "        num_categories=dados_prep['dims']['num_categories'],\n",
    "        num_subcategories=dados_prep['dims']['num_subcategories'],\n",
    "        embedding_dim=50,\n",
    "        l2_strength=0.001,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        dados_prep['X_train'],\n",
    "        dados_prep['y_train'],\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Avaliar modelo\n",
    "    loss = model.evaluate(dados_prep['X_test'], dados_prep['y_test'], verbose=0)\n",
    "    print(f\"Teste MSE: {loss}\")\n",
    "    \n",
    "    # Registrar modelo no MLflow\n",
    "    metrics = {\"mse\": loss}\n",
    "    params = {\n",
    "        \"embedding_dim\": 50,\n",
    "        \"l2_strength\": 0.001,\n",
    "        \"dropout_rate\": 0.2\n",
    "    }\n",
    "    \n",
    "    run_id = log_model_metrics(model, metrics, params)\n",
    "    print(f\"Modelo registrado com run_id: {run_id}\")\n",
    "else:\n",
    "    print(\"Modelo carregado do registro MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funções Avançadas de SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_shap_explainer(model, background_data, nsamples=500):\n",
    "    \"\"\"\n",
    "    Cria um explainer SHAP para o modelo de recomendação.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo treinado\n",
    "        background_data: Dados de background para o explainer\n",
    "        nsamples: Número de amostras para o KernelExplainer\n",
    "        \n",
    "    Returns:\n",
    "        Explainer SHAP\n",
    "    \"\"\"\n",
    "    # Função para prever com o modelo\n",
    "    def model_predict(X):\n",
    "        # Extrair features do formato esperado pelo KernelExplainer\n",
    "        batch_size = X.shape[0]\n",
    "        customer_ids = X[:, 0].astype(int)\n",
    "        product_ids = X[:, 1].astype(int)\n",
    "        category_ids = X[:, 2].astype(int)\n",
    "        subcategory_ids = X[:, 3].astype(int)\n",
    "        \n",
    "        # Prever\n",
    "        return model.predict(\n",
    "            [customer_ids, product_ids, category_ids, subcategory_ids],\n",
    "            verbose=0\n",
    "        ).flatten()\n",
    "    \n",
    "    # Criar explainer\n",
    "    explainer = shap.KernelExplainer(model_predict, background_data, nsamples=nsamples)\n",
    "    return explainer\n",
    "\n",
    "def prepare_shap_data(X_test, num_samples=100):\n",
    "    \"\"\"\n",
    "    Prepara dados para análise SHAP.\n",
    "    \n",
    "    Args:\n",
    "        X_test: Dados de teste [customer_ids, product_ids, category_ids, subcategory_ids]\n",
    "        num_samples: Número de amostras a selecionar\n",
    "        \n",
    "    Returns:\n",
    "        Dados formatados para SHAP\n",
    "    \"\"\"\n",
    "    # Selecionar amostras aleatórias\n",
    "    indices = np.random.choice(len(X_test[0]), min(num_samples, len(X_test[0])), replace=False)\n",
    "    \n",
    "    # Extrair dados\n",
    "    customer_ids = X_test[0][indices]\n",
    "    product_ids = X_test[1][indices]\n",
    "    category_ids = X_test[2][indices]\n",
    "    subcategory_ids = X_test[3][indices]\n",
    "    \n",
    "    # Combinar em um array\n",
    "    X_shap = np.column_stack([customer_ids, product_ids, category_ids, subcategory_ids])\n",
    "    \n",
    "    return X_shap, indices\n",
    "\n",
    "def get_feature_names(dados_prep):\n",
    "    \"\"\"\n",
    "    Obtém nomes das features para visualizações SHAP.\n",
    "    \n",
    "    Args:\n",
    "        dados_prep: Dados preparados\n",
    "        \n",
    "    Returns:\n",
    "        Lista de nomes de features\n",
    "    \"\"\"\n",
    "    return [\"Cliente\", \"Produto\", \"Categoria\", \"Subcategoria\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparar Dados para SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preparar dados para SHAP\n",
    "background_data, _ = prepare_shap_data(dados_prep['X_train'], num_samples=200)\n",
    "test_data, test_indices = prepare_shap_data(dados_prep['X_test'], num_samples=100)\n",
    "\n",
    "# Criar explainer\n",
    "explainer = create_shap_explainer(model, background_data, nsamples=500)\n",
    "\n",
    "# Calcular valores SHAP\n",
    "shap_values = explainer.shap_values(test_data)\n",
    "\n",
    "# Obter nomes das features\n",
    "feature_names = get_feature_names(dados_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizações Avançadas de SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5.1 Summary Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, test_data, feature_names=feature_names, show=False)\n",
    "plt.title(\"Impacto das Features nas Recomendações\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/figuras/shap_summary_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5.2 Bar Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, test_data, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Importância Global das Features\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/figuras/shap_bar_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5.3 Dependence Plot para cada feature\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.dependence_plot(i, shap_values, test_data, feature_names=feature_names, show=False)\n",
    "    plt.title(f\"Dependence Plot: {feature}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../reports/figuras/shap_dependence_{feature}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5.4 Force Plot para uma instância específica\n",
    "# Selecionar uma instância aleatória\n",
    "instance_idx = np.random.randint(0, len(test_data))\n",
    "instance = test_data[instance_idx:instance_idx+1]\n",
    "\n",
    "# Obter cliente e produto originais\n",
    "customer_id = dados_prep['encoders']['customer'].inverse_transform([int(instance[0, 0])])[0]\n",
    "product_id = dados_prep['encoders']['product'].inverse_transform([int(instance[0, 1])])[0]\n",
    "category = dados_prep['encoders']['category'].inverse_transform([int(instance[0, 2])])[0]\n",
    "subcategory = dados_prep['encoders']['subcategory'].inverse_transform([int(instance[0, 3])])[0]\n",
    "\n",
    "print(f\"Cliente: {customer_id}\")\n",
    "print(f\"Produto: {product_id}\")\n",
    "print(f\"Categoria: {category}\")\n",
    "print(f\"Subcategoria: {subcategory}\")\n",
    "\n",
    "# Criar force plot\n",
    "plt.figure(figsize=(20, 3))\n",
    "force_plot = shap.force_plot(explainer.expected_value, shap_values[instance_idx], instance[0], \n",
    "                             feature_names=feature_names, matplotlib=True, show=False)\n",
    "plt.title(f\"Explicação da Recomendação para Cliente {customer_id}, Produto {product_id}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../reports/figuras/shap_force_plot_{customer_id}_{product_id}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise por Grupos de Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Agrupar clientes por volume de compras\n",
    "def analisar_grupos_clientes(dados_prep, explainer, model):\n",
    "    \"\"\"\n",
    "    Analisa explicabilidade por grupos de clientes.\n",
    "    \"\"\"\n",
    "    # Calcular volume de compras por cliente\n",
    "    dados = dados_prep['dados_originais']\n",
    "    volume_por_cliente = dados.groupby('customer_id')['sales'].sum().reset_index()\n",
    "    \n",
    "    # Definir quartis\n",
    "    quartis = volume_por_cliente['sales'].quantile([0.25, 0.5, 0.75]).values\n",
    "    \n",
    "    # Categorizar clientes\n",
    "    def categorizar_cliente(volume):\n",
    "        if volume <= quartis[0]:\n",
    "            return \"Baixo Volume\"\n",
    "        elif volume <= quartis[1]:\n",
    "            return \"Médio-Baixo Volume\"\n",
    "        elif volume <= quartis[2]:\n",
    "            return \"Médio-Alto Volume\"\n",
    "        else:\n",
    "            return \"Alto Volume\"\n",
    "    \n",
    "    volume_por_cliente['grupo'] = volume_por_cliente['sales'].apply(categorizar_cliente)\n",
    "    \n",
    "    # Mesclar com dados originais\n",
    "    dados_com_grupo = dados.merge(volume_por_cliente[['customer_id', 'grupo']], on='customer_id')\n",
    "    \n",
    "    # Analisar cada grupo\n",
    "    grupos = ['Baixo Volume', 'Médio-Baixo Volume', 'Médio-Alto Volume', 'Alto Volume']\n",
    "    resultados = {}\n",
    "    \n",
    "    for grupo in grupos:\n",
    "        print(f\"\\nAnalisando grupo: {grupo}\")\n",
    "        \n",
    "        # Filtrar dados do grupo\n",
    "        dados_grupo = dados_com_grupo[dados_com_grupo['grupo'] == grupo]\n",
    "        \n",
    "        # Selecionar amostra aleatória\n",
    "        if len(dados_grupo) > 50:\n",
    "            dados_grupo = dados_grupo.sample(50, random_state=42)\n",
    "        \n",
    "        # Preparar dados para SHAP\n",
    "        customer_ids = dados_prep['encoders']['customer'].transform(dados_grupo['customer_id'])\n",
    "        product_ids = dados_prep['encoders']['product'].transform(dados_grupo['product_id'])\n",
    "        category_ids = dados_prep['encoders']['category'].transform(dados_grupo['category'])\n",
    "        subcategory_ids = dados_prep['encoders']['subcategory'].transform(dados_grupo['subcategory'])\n",
    "        \n",
    "        X_grupo = np.column_stack([customer_ids, product_ids, category_ids, subcategory_ids])\n",
    "        \n",
    "        # Calcular valores SHAP\n",
    "        shap_values_grupo = explainer.shap_values(X_grupo)\n",
    "        \n",
    "        # Calcular importância média das features\n",
    "        importancia_media = np.abs(shap_values_grupo).mean(axis=0)\n",
    "        \n",
    "        # Armazenar resultados\n",
    "        resultados[grupo] = {\n",
    "            'shap_values': shap_values_grupo,\n",
    "            'X': X_grupo,\n",
    "            'importancia_media': importancia_media\n",
    "        }\n",
    "        \n",
    "        # Mostrar importância média\n",
    "        print(\"Importância média das features:\")\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            print(f\"{feature}: {importancia_media[i]:.4f}\")\n",
    "    \n",
    "    # Visualizar comparação entre grupos\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Preparar dados para o gráfico\n",
    "    importancias = []\n",
    "    for grupo in grupos:\n",
    "        importancias.append(resultados[grupo]['importancia_media'])\n",
    "    \n",
    "    importancias = np.array(importancias)\n",
    "    \n",
    "    # Criar gráfico de barras agrupadas\n",
    "    x = np.arange(len(feature_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, grupo in enumerate(grupos):\n",
    "        plt.bar(x + i*width - 0.3, importancias[i], width, label=grupo)\n",
    "    \n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importância SHAP Média')\n",
    "    plt.title('Comparação da Importância das Features por Grupo de Clientes')\n",
    "    plt.xticks(x, feature_names)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../reports/figuras/shap_comparacao_grupos.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Executar análise por grupos\n",
    "resultados_grupos = analisar_grupos_clientes(dados_prep, explainer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Geração de Relatório Personalizado Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def gerar_relatorio_personalizado(cliente_id, model, explainer, dados_prep, num_recomendacoes=5):\n",
    "    \"\"\"\n",
    "    Gera um relatório personalizado com recomendações e explicações SHAP para um cliente específico.\n",
    "    \n",
    "    Args:\n",
    "        cliente_id: ID do cliente\n",
    "        model: Modelo treinado\n",
    "        explainer: Explainer SHAP\n",
    "        dados_prep: Dados preparados\n",
    "        num_recomendacoes: Número de recomendações a gerar\n",
    "    \"\"\"\n",
    "    # Verificar se o cliente existe\n",
    "    dados = dados_prep['dados_originais']\n",
    "    if cliente_id not in dados['customer_id'].unique():\n",
    "        print(f\"Cliente {cliente_id} não encontrado nos dados.\")\n",
    "        return\n",
    "    \n",
    "    # Obter dados do cliente\n",
    "    dados_cliente = dados[dados['customer_id'] == cliente_id]\n",
    "    \n",
    "    # Obter produtos já comprados pelo cliente\n",
    "    produtos_comprados = set(dados_cliente['product_id'].unique())\n",
    "    \n",
    "    # Obter todos os produtos disponíveis\n",
    "    todos_produtos = set(dados['product_id'].unique())\n",
    "    \n",
    "    # Produtos que o cliente ainda não comprou\n",
    "    produtos_nao_comprados = list(todos_produtos - produtos_comprados)\n",
    "    \n",
    "    # Preparar dados para predição\n",
    "    customer_id_encoded = dados_prep['encoders']['customer'].transform([cliente_id])[0]\n",
    "    product_ids_encoded = dados_prep['encoders']['product'].transform(produtos_nao_comprados)\n",
    "    \n",
    "    # Obter categorias e subcategorias para cada produto\n",
    "    categorias = []\n",
    "    subcategorias = []\n",
    "    \n",
    "    for produto in produtos_nao_comprados:\n",
    "        produto_info = dados[dados['product_id'] == produto].iloc[0]\n",
    "        categorias.append(produto_info['category'])\n",
    "        subcategorias.append(produto_info['subcategory'])\n",
    "    \n",
    "    category_ids_encoded = dados_prep['encoders']['category'].transform(categorias)\n",
    "    subcategory_ids_encoded = dados_prep['encoders']['subcategory'].transform(subcategorias)\n",
    "    \n",
    "    # Criar arrays para predição\n",
    "    customer_array = np.full(len(produtos_nao_comprados), customer_id_encoded)\n",
    "    \n",
    "    # Prever scores\n",
    "    scores = model.predict(\n",
    "        [customer_array, product_ids_encoded, category_ids_encoded, subcategory_ids_encoded],\n",
    "        verbose=0\n",
    "    ).flatten()\n",
    "    \n",
    "    # Obter top-k produtos recomendados\n",
    "    top_indices = np.argsort(scores)[::-1][:num_recomendacoes]\n",
    "    top_produtos = [produtos_nao_comprados[i] for i in top_indices]\n",
    "    top_scores = scores[top_indices]\n",
    "    top_categorias = [categorias[i] for i in top_indices]\n",
    "    top_subcategorias = [subcategorias[i] for i in top_indices]\n",
    "    \n",
    "    # Preparar dados para SHAP\n",
    "    X_shap = []\n",
    "    for i in range(len(top_produtos)):\n",
    "        X_shap.append([\n",
    "            customer_id_encoded,\n",
    "            dados_prep['encoders']['product'].transform([top_produtos[i]])[0],\n",
    "            dados_prep['encoders']['category'].transform([top_categorias[i]])[0],\n",
    "            dados_prep['encoders']['subcategory'].transform([top_subcategorias[i]])[0]\n",
    "        ])\n",
    "    \n",
    "    X_shap = np.array(X_shap)\n",
    "    \n",
    "    # Calcular valores SHAP\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    \n",
    "    # Criar PDF\n",
    "    pdf_path = f\"../reports/relatorio_cliente_{cliente_id}.pdf\"\n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    elements = []\n",
    "    \n",
    "    # Título\n",
    "    title_style = styles['Title']\n",
    "    elements.append(Paragraph(f\"Relatório de Recomendações Personalizado\", title_style))\n",
    "    elements.append(Spacer(1, 12))\n",
    "    \n",
    "    # Informações do cliente\n",
    "    elements.append(Paragraph(f\"Cliente: {cliente_id}\", styles['Heading2']))\n",
    "    elements.append(Spacer(1, 12))\n",
    "    \n",
    "    # Histórico de compras\n",
    "    elements.append(Paragraph(\"Histórico de Compras\", styles['Heading2']))\n",
    "    elements.append(Spacer(1, 6))\n",
    "    \n",
    "    # Tabela com histórico de compras\n",
    "    historico = dados_cliente.sort_values('sales', ascending=False).head(10)\n",
    "    data = [[\"Produto\", \"Categoria\", \"Subcategoria\", \"Vendas\"]]\n",
    "    for _, row in historico.iterrows():\n",
    "        data.append([row['product_id'], row['category'], row['subcategory'], f\"{row['sales']:.2f}\"])\n",
    "    \n",
    "    table = Table(data)\n",
    "    table.setStyle(TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "    ]))\n",
    "    elements.append(table)\n",
    "    elements.append(Spacer(1, 12))\n",
    "    \n",
    "    # Recomendações\n",
    "    elements.append(Paragraph(\"Produtos Recomendados\", styles['Heading2']))\n",
    "    elements.append(Spacer(1, 6))\n",
    "    \n",
    "    # Para cada recomendação\n",
    "    for i in range(len(top_produtos)):\n",
    "        # Informações do produto\n",
    "        elements.append(Paragraph(f\"Recomendação #{i+1}: {top_produtos[i]}\", styles['Heading3']))\n",
    "        elements.append(Paragraph(f\"Categoria: {top_categorias[i]}\", styles['Normal']))\n",
    "        elements.append(Paragraph(f\"Subcategoria: {top_subcategorias[i]}\", styles['Normal']))\n",
    "        elements.append(Paragraph(f\"Score: {top_scores[i]:.4f}\", styles['Normal']))\n",
    "        elements.append(Spacer(1, 6))\n",
    "        \n",
    "        # Gerar force plot\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        shap.force_plot(\n",
    "            explainer.expected_value, \n",
    "            shap_values[i], \n",
    "            X_shap[i], \n",
    "            feature_names=feature_names,\n",
    "            matplotlib=True,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f\"Explicação da Recomendação\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salvar figura em buffer\n",
    "        img_buffer = BytesIO()\n",
    "        plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Adicionar imagem ao PDF\n",
    "        img_buffer.seek(0)\n",
    "        img = Image(img_buffer, width=450, height=100)\n",
    "        elements.append(img)\n",
    "        elements.append(Spacer(1, 12))\n",
    "        \n",
    "        # Explicação textual\n",
    "        elements.append(Paragraph(\"Explicação:\", styles['Heading4']))\n",
    "        \n",
    "        # Ordenar features por importância\n",
    "        feature_importance = [(feature_names[j], abs(shap_values[i][j])) for j in range(len(feature_names))]\n",
    "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for feature, importance in feature_importance:\n",
    "            direction = \"aumenta\" if shap_values[i][feature_names.index(feature)] > 0 else \"diminui\"\n",
    "            elements.append(Paragraph(\n",
    "                f\"• {feature}: {direction} a probabilidade de recomendação (impacto: {importance:.4f})\",\n",
    "                styles['Normal']\n",
    "            ))\n",
    "        \n",
    "        elements.append(Spacer(1, 12))\n",
    "    \n",
    "    # Resumo geral\n",
    "    elements.append(Paragraph(\"Resumo da Análise\", styles['Heading2']))\n",
    "    elements.append(Spacer(1, 6))\n",
    "    \n",
    "    # Gerar bar plot com importância média das features\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    importancia_media = np.abs(shap_values).mean(axis=0)\n",
    "    plt.bar(feature_names, importancia_media)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importância SHAP Média')\n",
    "    plt.title('Importância Média das Features nas Recomendações')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salvar figura em buffer\n",
    "    img_buffer = BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Adicionar imagem ao PDF\n",
    "    img_buffer.seek(0)\n",
    "    img = Image(img_buffer, width=400, height=250)\n",
    "    elements.append(img)\n",
    "    elements.append(Spacer(1, 12))\n",
    "    \n",
    "    # Conclusão\n",
    "    elements.append(Paragraph(\"Conclusão\", styles['Heading2']))\n",
    "    elements.append(Spacer(1, 6))\n",
    "    \n",
    "    # Determinar feature mais importante\n",
    "    feature_mais_importante = feature_names[np.argmax(importancia_media)]\n",
    "    \n",
    "    elements.append(Paragraph(\n",
    "        f\"Com base na análise SHAP, a feature '{feature_mais_importante}' tem o maior impacto nas recomendações \"\n",
    "        f\"para este cliente. As recomendações foram geradas considerando o histórico de compras do cliente \"\n",
    "        f\"e padrões identificados pelo modelo de recomendação.\",\n",
    "        styles['Normal']\n",
    "    ))\n",
    "    \n",
    "    # Construir PDF\n",
    "    doc.build(elements)\n",
    "    \n",
    "    print(f\"Relatório gerado com sucesso: {pdf_path}\")\n",
    "    return pdf_path\n",
    "\n",
    "# Gerar relatório para um cliente específico\n",
    "# Selecionar um cliente aleatório\n",
    "cliente_aleatorio = np.random.choice(dados['customer_id'].unique())\n",
    "relatorio_path = gerar_relatorio_personalizado(cliente_aleatorio, model, explainer, dados_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparação de Modelos com SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def comparar_modelos_com_shap(model_original, model_regularizado, dados_prep, test_data):\n",
    "    \"\"\"\n",
    "    Compara dois modelos usando SHAP.\n",
    "    \"\"\"\n",
    "    # Preparar dados de background\n",
    "    background_data, _ = prepare_shap_data(dados_prep['X_train'], num_samples=200)\n",
    "    \n",
    "    # Criar explainers\n",
    "    explainer_original = create_shap_explainer(model_original, background_data, nsamples=300)\n",
    "    explainer_regularizado = create_shap_explainer(model_regularizado, background_data, nsamples=300)\n",
    "    \n",
    "    # Calcular valores SHAP\n",
    "    shap_values_original = explainer_original.shap_values(test_data)\n",
    "    shap_values_regularizado = explainer_regularizado.shap_values(test_data)\n",
    "    \n",
    "    # Calcular importância média das features\n",
    "    importancia_original = np.abs(shap_values_original).mean(axis=0)\n",
    "    importancia_regularizado = np.abs(shap_values_regularizado).mean(axis=0)\n",
    "    \n",
    "    # Visualizar comparação\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(feature_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, importancia_original, width, label='Modelo Original')\n",
    "    plt.bar(x + width/2, importancia_regularizado, width, label='Modelo Regularizado')\n",
    "    \n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importância SHAP Média')\n",
    "    plt.title('Comparação da Importância das Features entre Modelos')\n",
    "    plt.xticks(x, feature_names)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../reports/figuras/shap_comparacao_modelos.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular diferença de importância\n",
    "    diferenca = importancia_regularizado - importancia_original\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(feature_names, diferenca, color=['green' if d > 0 else 'red' for d in diferenca])\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Diferença de Importância (Regularizado - Original)')\n",
    "    plt.title('Mudança na Importância das Features após Regularização')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../reports/figuras/shap_diferenca_modelos.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'importancia_original': importancia_original,\n",
    "        'importancia_regularizado': importancia_regularizado,\n",
    "        'diferenca': diferenca\n",
    "    }\n",
    "\n",
    "# Nota: Para executar esta função, seria necessário ter dois modelos diferentes para comparação.\n",
    "# Como exemplo, podemos criar um modelo sem regularização para comparar com o modelo regularizado.\n",
    "# Este código está comentado pois depende de ter dois modelos disponíveis.\n",
    "\n",
    "# model_original = load_registered_model(model_name=\"recommendation_model_original\", stage=\"Production\")\n",
    "# if model_original:\n",
    "#     resultados_comparacao = comparar_modelos_com_shap(model_original, model, dados_prep, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusão e Próximos Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook, exploramos técnicas avançadas de explicabilidade para o modelo de recomendação utilizando SHAP. As principais conclusões são:\n",
    "\n",
    "1. Visualizações avançadas de SHAP permitem entender melhor o comportamento do modelo\n",
    "2. A análise por grupos de clientes revela diferenças na importância das features para diferentes perfis\n",
    "3. Os relatórios personalizados fornecem insights detalhados sobre as recomendações para cada cliente\n",
    "4. A comparação entre modelos ajuda a entender o impacto da regularização na interpretabilidade\n",
    "\n",
    "Próximos passos:\n",
    "\n",
    "1. Integrar estas análises ao pipeline de MLOps\n",
    "2. Automatizar a geração de relatórios para clientes importantes\n",
    "3. Explorar técnicas adicionais de explicabilidade, como LIME ou Integrated Gradients\n",
    "4. Desenvolver um dashboard interativo para visualização das explicações SHAP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}