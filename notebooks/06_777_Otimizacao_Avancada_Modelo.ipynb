{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Sistema de Recomendação - Otimização Avançada\n",
    "\n",
    "Este notebook implementa técnicas avançadas para maximizar a performance do modelo:\n",
    "- Validação cruzada\n",
    "- Otimização de hiperparâmetros\n",
    "- Métricas avançadas de recomendação\n",
    "- Regularização e técnicas anti-overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do ambiente\n",
    "%cd ..\n",
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importar utilitários personalizados\n",
    "from src.config.paths import DADOS_BRUTOS, DADOS_TRATADOS\n",
    "from src.config.auxiliares_ml import downcast_dataframe\n",
    "from src.config.model_utils import (\n",
    "    create_model_with_regularization,\n",
    "    cross_validate_model,\n",
    "    calculate_recommendation_metrics,\n",
    "    hyperparameter_tuning\n",
    ")\n",
    "from mlflow_tracking import setup_mlflow, log_model_metrics\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar MLflow\n",
    "setup_mlflow()\n",
    "print(\"MLflow configurado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar e preparar os dados\n",
    "print(\"Carregando dados...\")\n",
    "df = pd.read_parquet(DADOS_TRATADOS)\n",
    "\n",
    "# Filtrar colunas relevantes\n",
    "df = df[['Customer Name', 'Product ID', 'Product Name', 'Sales', 'Category', 'Sub-Category']]\n",
    "\n",
    "print(f\"Dados carregados: {df.shape}\")\n",
    "print(f\"Clientes únicos: {df['Customer Name'].nunique()}\")\n",
    "print(f\"Produtos únicos: {df['Product ID'].nunique()}\")\n",
    "print(f\"Categorias únicas: {df['Category'].nunique()}\")\n",
    "print(f\"Sub-categorias únicas: {df['Sub-Category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar variáveis categóricas\n",
    "print(\"Codificando variáveis categóricas...\")\n",
    "\n",
    "customer_encoder = LabelEncoder()\n",
    "product_encoder = LabelEncoder()\n",
    "category_encoder = LabelEncoder()\n",
    "subcategory_encoder = LabelEncoder()\n",
    "\n",
    "df['Customer ID Enc'] = customer_encoder.fit_transform(df['Customer Name'])\n",
    "df['Product ID Enc'] = product_encoder.fit_transform(df['Product ID'])\n",
    "df['Category Enc'] = category_encoder.fit_transform(df['Category'])\n",
    "df['Sub-Category Enc'] = subcategory_encoder.fit_transform(df['Sub-Category'])\n",
    "\n",
    "# Normalizar vendas\n",
    "scaler = MinMaxScaler()\n",
    "df['Sales Normalized'] = scaler.fit_transform(df[['Sales']])\n",
    "\n",
    "print(\"Codificação concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para o modelo\n",
    "customer_ids = df['Customer ID Enc'].values\n",
    "product_ids = df['Product ID Enc'].values\n",
    "category_ids = df['Category Enc'].values\n",
    "subcategory_ids = df['Sub-Category Enc'].values\n",
    "sales = df['Sales Normalized'].values\n",
    "\n",
    "# Dimensões dos embeddings\n",
    "num_customers = len(customer_encoder.classes_)\n",
    "num_products = len(product_encoder.classes_)\n",
    "num_categories = len(category_encoder.classes_)\n",
    "num_subcategories = len(subcategory_encoder.classes_)\n",
    "\n",
    "print(f\"Dimensões dos embeddings:\")\n",
    "print(f\"Clientes: {num_customers}\")\n",
    "print(f\"Produtos: {num_products}\")\n",
    "print(f\"Categorias: {num_categories}\")\n",
    "print(f\"Sub-categorias: {num_subcategories}\")\n",
    "\n",
    "# Preparar dados para validação cruzada\n",
    "X = (customer_ids, product_ids, category_ids, subcategory_ids)\n",
    "y = sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 1. Otimização de Hiperparâmetros\n",
    "\n",
    "Vamos encontrar os melhores hiperparâmetros para o modelo usando grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar otimização de hiperparâmetros\n",
    "print(\"Iniciando otimização de hiperparâmetros...\")\n",
    "print(\"Isso pode levar alguns minutos...\")\n",
    "\n",
    "best_params = hyperparameter_tuning(\n",
    "    X, y, num_customers, num_products, num_categories, num_subcategories\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MELHORES HIPERPARÂMETROS ENCONTRADOS:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2. Validação Cruzada com Melhores Parâmetros\n",
    "\n",
    "Agora vamos treinar o modelo com os melhores hiperparâmetros usando validação cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar validação cruzada com os melhores parâmetros\n",
    "print(\"Executando validação cruzada com os melhores parâmetros...\")\n",
    "\n",
    "cv_results = cross_validate_model(\n",
    "    X, y, \n",
    "    num_customers, num_products, num_categories, num_subcategories,\n",
    "    embedding_dim=best_params.get('embedding_dim', 50),\n",
    "    n_splits=5,\n",
    "    epochs=25,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTADOS DA VALIDAÇÃO CRUZADA:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MSE Médio: {cv_results['mean_mse']:.6f} ± {cv_results['std_mse']:.6f}\")\n",
    "print(f\"Val Loss Médio: {cv_results['mean_val_loss']:.6f} ± {cv_results['std_val_loss']:.6f}\")\n",
    "print(\"\\nResultados por fold:\")\n",
    "for fold_result in cv_results['fold_metrics']:\n",
    "    print(f\"Fold {fold_result['fold']}: MSE = {fold_result['mse']:.6f}, Val Loss = {fold_result['val_loss']:.6f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 3. Treinamento do Modelo Final Otimizado\n",
    "\n",
    "Vamos treinar o modelo final com todos os dados usando os melhores hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar modelo final com os melhores parâmetros\n",
    "print(\"Criando modelo final otimizado...\")\n",
    "\n",
    "final_model = create_model_with_regularization(\n",
    "    num_customers, num_products, num_categories, num_subcategories,\n",
    "    embedding_dim=best_params.get('embedding_dim', 50),\n",
    "    l2_strength=best_params.get('l2_strength', 0.001),\n",
    "    dropout_rate=best_params.get('dropout_rate', 0.2)\n",
    ")\n",
    "\n",
    "# Ajustar learning rate\n",
    "final_model.optimizer.learning_rate = best_params.get('learning_rate', 0.0005)\n",
    "\n",
    "print(\"Modelo criado com sucesso!\")\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dados para treinamento final\n",
    "(\n",
    "    customer_train, customer_test,\n",
    "    product_train, product_test,\n",
    "    category_train, category_test,\n",
    "    subcategory_train, subcategory_test,\n",
    "    sales_train, sales_test\n",
    ") = train_test_split(\n",
    "    customer_ids, product_ids, category_ids, subcategory_ids, sales,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dados de treino: {len(customer_train)}\")\n",
    "print(f\"Dados de teste: {len(customer_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo final\n",
    "print(\"Treinando modelo final...\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_otimizado\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Treinar\n",
    "history = final_model.fit(\n",
    "    [customer_train.reshape(-1, 1), product_train.reshape(-1, 1), \n",
    "     category_train.reshape(-1, 1), subcategory_train.reshape(-1, 1)],\n",
    "    sales_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(\n",
    "        [customer_test.reshape(-1, 1), product_test.reshape(-1, 1),\n",
    "         category_test.reshape(-1, 1), subcategory_test.reshape(-1, 1)],\n",
    "        sales_test\n",
    "    ),\n",
    "    callbacks=[early_stopping, reduce_lr, tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Treinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 4. Avaliação Avançada do Modelo\n",
    "\n",
    "Vamos calcular métricas avançadas de recomendação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para métricas de recomendação\n",
    "print(\"Preparando dados para métricas de recomendação...\")\n",
    "\n",
    "# Criar dicionário de compras reais por cliente\n",
    "actual_purchases = {}\n",
    "for idx, row in df.iterrows():\n",
    "    customer_id = row['Customer ID Enc']\n",
    "    product_id = row['Product ID']\n",
    "    \n",
    "    if customer_id not in actual_purchases:\n",
    "        actual_purchases[customer_id] = []\n",
    "    \n",
    "    if product_id not in actual_purchases[customer_id]:\n",
    "        actual_purchases[customer_id].append(product_id)\n",
    "\n",
    "print(f\"Dados preparados para {len(actual_purchases)} clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de recomendação\n",
    "print(\"Calculando métricas de recomendação...\")\n",
    "\n",
    "# Usar uma amostra para acelerar o cálculo\n",
    "sample_size = min(1000, len(customer_test))\n",
    "sample_indices = np.random.choice(len(customer_test), sample_size, replace=False)\n",
    "\n",
    "customer_sample = customer_test[sample_indices]\n",
    "product_sample = product_test[sample_indices]\n",
    "category_sample = category_test[sample_indices]\n",
    "subcategory_sample = subcategory_test[sample_indices]\n",
    "\n",
    "recommendation_metrics = calculate_recommendation_metrics(\n",
    "    final_model,\n",
    "    customer_sample,\n",
    "    product_sample,\n",
    "    category_sample,\n",
    "    subcategory_sample,\n",
    "    actual_purchases,\n",
    "    product_encoder,\n",
    "    top_k=7\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MÉTRICAS DE RECOMENDAÇÃO:\")\n",
    "print(\"=\"*50)\n",
    "for metric_name, metric_value in recommendation_metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas finais do modelo\n",
    "print(\"Calculando métricas finais...\")\n",
    "\n",
    "# Predições no conjunto de teste\n",
    "test_predictions = final_model.predict([\n",
    "    customer_test.reshape(-1, 1),\n",
    "    product_test.reshape(-1, 1),\n",
    "    category_test.reshape(-1, 1),\n",
    "    subcategory_test.reshape(-1, 1)\n",
    "], verbose=0).flatten()\n",
    "\n",
    "# MSE e RMSE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(sales_test, test_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(sales_test, test_predictions)\n",
    "r2 = r2_score(sales_test, test_predictions)\n",
    "\n",
    "final_metrics = {\n",
    "    'mse': mse,\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'r2_score': r2,\n",
    "    **recommendation_metrics\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MÉTRICAS FINAIS DO MODELO OTIMIZADO:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(\"\\nMétricas de Recomendação:\")\n",
    "for metric_name, metric_value in recommendation_metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 5. Registro no MLflow\n",
    "\n",
    "Vamos registrar o modelo otimizado e suas métricas no MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar modelo otimizado no MLflow\n",
    "print(\"Registrando modelo otimizado no MLflow...\")\n",
    "\n",
    "X_test_formatted = [\n",
    "    customer_test.reshape(-1, 1),\n",
    "    product_test.reshape(-1, 1),\n",
    "    category_test.reshape(-1, 1),\n",
    "    subcategory_test.reshape(-1, 1)\n",
    "]\n",
    "\n",
    "run_id = log_model_metrics(\n",
    "    final_model,\n",
    "    X_test_formatted,\n",
    "    sales_test,\n",
    "    final_metrics,\n",
    "    model_name=\"recommendation_model_optimized\"\n",
    ")\n",
    "\n",
    "print(f\"Modelo registrado com sucesso! Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 6. Salvar Modelo Otimizado\n",
    "\n",
    "Vamos salvar o modelo otimizado para uso em produção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo otimizado\n",
    "model_path = \"models/best_model_recomendacao_otimizado.keras\"\n",
    "final_model.save(model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")\n",
    "\n",
    "# Salvar encoders\n",
    "import joblib\n",
    "\n",
    "joblib.dump(customer_encoder, \"models/customer_encoder_otimizado.pkl\")\n",
    "joblib.dump(product_encoder, \"models/product_encoder_otimizado.pkl\")\n",
    "joblib.dump(category_encoder, \"models/category_encoder_otimizado.pkl\")\n",
    "joblib.dump(subcategory_encoder, \"models/subcategory_encoder_otimizado.pkl\")\n",
    "joblib.dump(scaler, \"models/sales_scaler_otimizado.pkl\")\n",
    "\n",
    "print(\"Encoders e scaler salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 7. Comparação com Modelo Anterior\n",
    "\n",
    "Vamos comparar a performance do modelo otimizado com o modelo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para testar recomendações\n",
    "def recomendar_produtos_otimizado(customer_name, df, model, customer_encoder, product_encoder, num_products=7):\n",
    "    \"\"\"\n",
    "    Função otimizada para recomendar produtos usando o modelo melhorado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Codificar o nome do cliente\n",
    "        customer_id_enc = customer_encoder.transform([customer_name])[0]\n",
    "    except ValueError:\n",
    "        return f\"Cliente '{customer_name}' não encontrado na base de dados.\"\n",
    "    \n",
    "    # Obter todos os produtos únicos\n",
    "    unique_products = df[['Product ID', 'Product Name', 'Category', 'Sub-Category']].drop_duplicates()\n",
    "    \n",
    "    # Preparar dados para predição\n",
    "    num_unique_products = len(unique_products)\n",
    "    customer_array = np.full(num_unique_products, customer_id_enc)\n",
    "    \n",
    "    # Codificar produtos, categorias e subcategorias\n",
    "    product_ids_enc = product_encoder.transform(unique_products['Product ID'])\n",
    "    category_ids_enc = category_encoder.transform(unique_products['Category'])\n",
    "    subcategory_ids_enc = subcategory_encoder.transform(unique_products['Sub-Category'])\n",
    "    \n",
    "    # Fazer predições\n",
    "    predictions = model.predict([\n",
    "        customer_array.reshape(-1, 1),\n",
    "        product_ids_enc.reshape(-1, 1),\n",
    "        category_ids_enc.reshape(-1, 1),\n",
    "        subcategory_ids_enc.reshape(-1, 1)\n",
    "    ], verbose=0).flatten()\n",
    "    \n",
    "    # Criar DataFrame com predições\n",
    "    recommendations_df = unique_products.copy()\n",
    "    recommendations_df['Prediction'] = predictions\n",
    "    \n",
    "    # Filtrar produtos já comprados pelo cliente\n",
    "    purchased_products = df[df['Customer Name'] == customer_name]['Product ID'].unique()\n",
    "    recommendations_df = recommendations_df[~recommendations_df['Product ID'].isin(purchased_products)]\n",
    "    \n",
    "    # Ordenar por predição e pegar top N\n",
    "    top_recommendations = recommendations_df.nlargest(num_products, 'Prediction')\n",
    "    \n",
    "    # Formatar resultado\n",
    "    result = top_recommendations[['Product ID', 'Product Name', 'Category', 'Sub-Category']].reset_index(drop=True)\n",
    "    result.insert(0, 'Ranking', range(1, len(result) + 1))\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Função de recomendação otimizada criada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar recomendações com o modelo otimizado\n",
    "print(\"Testando recomendações com o modelo otimizado...\")\n",
    "\n",
    "cliente_teste = \"Darrin Van Huff\"\n",
    "recomendacoes_otimizadas = recomendar_produtos_otimizado(\n",
    "    cliente_teste, df, final_model, customer_encoder, product_encoder, num_products=7\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"RECOMENDAÇÕES OTIMIZADAS PARA: {cliente_teste}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(recomendacoes_otimizadas)\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 8. Resumo das Otimizações\n",
    "\n",
    "### Técnicas Implementadas:\n",
    "\n",
    "1. **Otimização de Hiperparâmetros**: Grid search para encontrar os melhores parâmetros\n",
    "2. **Validação Cruzada**: Avaliação robusta com 5 folds\n",
    "3. **Regularização L2**: Prevenção de overfitting\n",
    "4. **Dropout**: Regularização adicional\n",
    "5. **Early Stopping**: Parada antecipada para evitar overfitting\n",
    "6. **Learning Rate Scheduling**: Redução automática da taxa de aprendizado\n",
    "7. **Métricas Avançadas**: Precision@k, Recall@k, F1@k\n",
    "8. **MLflow Tracking**: Rastreamento completo de experimentos\n",
    "\n",
    "### Melhorias Obtidas:\n",
    "- Modelo mais robusto e generalizado\n",
    "- Melhor performance em dados não vistos\n",
    "- Métricas de recomendação mais precisas\n",
    "- Rastreamento completo de experimentos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (TensorFlow)",
   "language": "python",
   "name": "meu_env_python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}