{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17292dc-0408-49cf-8df6-541e946aa8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flavi\\Documents\\GitHub\\Projeto_7_Sistema_de_Recomendacao\n",
      "C:\\Users\\flavi\\Documents\\GitHub\\Projeto_7_Sistema_de_Recomendacao\n"
     ]
    }
   ],
   "source": [
    "# Configuração do ambiente\n",
    "\n",
    "# Garante que o notebook está na raiz do projeto\n",
    "%cd .. \n",
    "\n",
    "# Verifica o diretório atual (Linux/Mac)\n",
    "# !pwd  \n",
    "\n",
    "# Verifica o diretório atual (Windows)\n",
    "!cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e504241f-8f5a-4485-9bc3-b41841c09865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações Necessárias\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.metrics import MeanSquaredError \n",
    "from pathlib import Path\n",
    "\n",
    "from src.config.paths import MODELS_DIR\n",
    "from src.config.paths import DADOS_TRATADOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac9b461-7ff5-4aa2-a996-776c018497c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carregamento e Pré-processamento dos Dados ---\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Carrega e pré-processa os dados.\"\"\"\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df = df[['Customer Name', 'Product ID', 'Product Name', 'Sales', 'Category', 'Sub-Category']].copy()\n",
    "\n",
    "    # Codificação\n",
    "    customer_encoder = LabelEncoder()\n",
    "    product_encoder = LabelEncoder()\n",
    "    category_encoder = LabelEncoder()\n",
    "    subcategory_encoder = LabelEncoder()\n",
    "\n",
    "    df['Customer ID Enc'] = customer_encoder.fit_transform(df['Customer Name'])\n",
    "    df['Product ID Enc'] = product_encoder.fit_transform(df['Product ID'])\n",
    "    df['Category Enc'] = category_encoder.fit_transform(df['Category'])\n",
    "    df['Sub-Category Enc'] = subcategory_encoder.fit_transform(df['Sub-Category'])\n",
    "\n",
    "    # Normalização\n",
    "    scaler = MinMaxScaler()\n",
    "    df['Sales Normalized'] = scaler.fit_transform(df[['Sales']])\n",
    "\n",
    "    return df, customer_encoder, product_encoder, category_encoder, subcategory_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19b365c-95d1-4011-8756-02cd9b240cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Divisão dos Dados ---\n",
    "\n",
    "def split_data(df):\n",
    "    \"\"\"Divide os dados em treino e teste.\"\"\"\n",
    "\n",
    "    customer_ids = df['Customer ID Enc'].values\n",
    "    product_ids = df['Product ID Enc'].values\n",
    "    category_ids = df['Category Enc'].values\n",
    "    subcategory_ids = df['Sub-Category Enc'].values\n",
    "    sales = df['Sales Normalized'].values\n",
    "\n",
    "    customer_ids_train, customer_ids_test, \\\n",
    "    product_ids_train, product_ids_test, \\\n",
    "    category_ids_train, category_ids_test, \\\n",
    "    subcategory_ids_train, subcategory_ids_test, \\\n",
    "    sales_train, sales_test = train_test_split(\n",
    "        customer_ids, product_ids, category_ids, subcategory_ids, sales,\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return (customer_ids_train, customer_ids_test,\n",
    "            product_ids_train, product_ids_test,\n",
    "            category_ids_train, category_ids_test,\n",
    "            subcategory_ids_train, subcategory_ids_test,\n",
    "            sales_train, sales_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9472cbff-bb01-495a-be22-c1d313f7b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modelo ---\n",
    "\n",
    "def create_model(num_customers, num_products, num_categories, num_subcategories, embedding_dim):\n",
    "    \"\"\"Cria o modelo de recomendação.\"\"\"\n",
    "\n",
    "    customer_input = layers.Input(shape=(1,), name='customer_input')\n",
    "    product_input = layers.Input(shape=(1,), name='product_input')\n",
    "    category_input = layers.Input(shape=(1,), name='category_input')\n",
    "    subcategory_input = layers.Input(shape=(1,), name='subcategory_input')\n",
    "\n",
    "    customer_embeddings = layers.Embedding(input_dim=num_customers, output_dim=embedding_dim, name='customer_embeddings')(customer_input)\n",
    "    product_embeddings = layers.Embedding(input_dim=num_products, output_dim=embedding_dim, name='product_embeddings')(product_input)\n",
    "    category_embeddings = layers.Embedding(input_dim=num_categories, output_dim=embedding_dim, name='category_embeddings')(category_input)\n",
    "    subcategory_embeddings = layers.Embedding(input_dim=num_subcategories, output_dim=embedding_dim, name='subcategory_embeddings')(subcategory_input)\n",
    "\n",
    "    customer_vec = layers.Flatten(name='customer_flatten')(customer_embeddings)\n",
    "    product_vec = layers.Flatten(name='product_flatten')(product_embeddings)\n",
    "    category_vec = layers.Flatten(name='category_flatten')(category_embeddings)\n",
    "    subcategory_vec = layers.Flatten(name='subcategory_flatten')(subcategory_embeddings)\n",
    "\n",
    "    concat_vec = layers.Concatenate(name='concat')([customer_vec, product_vec, category_vec, subcategory_vec])\n",
    "\n",
    "    dense_1 = layers.Dense(64, activation='relu', name='dense_1')(concat_vec)\n",
    "    dense_2 = layers.Dense(32, activation='relu', name='dense_2')(dense_1)\n",
    "    output = layers.Dense(1, activation='linear', name='output')(dense_2)\n",
    "\n",
    "    model = tf.keras.Model([customer_input, product_input, category_input, subcategory_input], output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')  # Otimizador Adam e taxa de aprendizado padrão\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d13444-d5af-4393-b731-17b3c2de41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Métricas de Avaliação ---\n",
    "\n",
    "def evaluate_model(model, customer_ids, product_ids, category_ids, subcategory_ids, sales, top_k=7):\n",
    "    \"\"\"Avalia o modelo usando MSE e métricas de recomendação (precisão@k, recall@k, F1@k).\"\"\"\n",
    "\n",
    "    predictions = model.predict([customer_ids, product_ids, category_ids, subcategory_ids], verbose=0).flatten()\n",
    "    mse_calculator = MeanSquaredError()  \n",
    "    mse = mse_calculator(sales, predictions).numpy()  \n",
    "\n",
    "    # Métricas de recomendação\n",
    "    relevant_items = defaultdict(list)\n",
    "    recommended_items = defaultdict(list)\n",
    "\n",
    "    for i in range(len(customer_ids)):\n",
    "        customer = customer_ids[i]\n",
    "        true_sale = sales[i]\n",
    "        prediction = predictions[i]\n",
    "\n",
    "        if true_sale > 0.5:  # Define o limiar para relevância \n",
    "            relevant_items[customer].append((product_ids[i], true_sale))\n",
    "\n",
    "        recommended_items[customer].append((product_ids[i], prediction))\n",
    "\n",
    "    precision_at_k_sum = 0\n",
    "    recall_at_k_sum = 0\n",
    "    f1_at_k_sum = 0\n",
    "    num_users_with_relevant_items = 0\n",
    "\n",
    "    for customer in relevant_items:\n",
    "        if len(relevant_items[customer]) > 0:\n",
    "            num_users_with_relevant_items += 1\n",
    "            # Sort recommended items by prediction score for this customer\n",
    "            recommended_items[customer].sort(key=lambda x: x[1], reverse=True)\n",
    "            top_k_recommended = [item[0] for item in recommended_items[customer][:top_k]]\n",
    "            \n",
    "            relevant_set = {item[0] for item in relevant_items[customer]}\n",
    "            \n",
    "            hits = len(relevant_set.intersection(top_k_recommended))\n",
    "            precision = hits / top_k if top_k else 0\n",
    "            recall = hits / len(relevant_set) if len(relevant_set) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            precision_at_k_sum += precision\n",
    "            recall_at_k_sum += recall\n",
    "            f1_at_k_sum += f1\n",
    "\n",
    "    avg_precision_at_k = precision_at_k_sum / num_users_with_relevant_items if num_users_with_relevant_items > 0 else 0\n",
    "    avg_recall_at_k = recall_at_k_sum / num_users_with_relevant_items if num_users_with_relevant_items > 0 else 0\n",
    "    avg_f1_at_k = f1_at_k_sum / num_users_with_relevant_items if num_users_with_relevant_items > 0 else 0\n",
    "\n",
    "    return mse, avg_precision_at_k, avg_recall_at_k, avg_f1_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d6460b-c1f7-421d-b0ea-2851e7b446f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função de Recomendação ---\n",
    "\n",
    "def recomendar_produtos(nome_cliente, df, model, customer_encoder, product_encoder, category_encoder, subcategory_encoder, top_k=7):\n",
    "    \"\"\"\n",
    "    Retorna os top-k produtos recomendados para um cliente específico,\n",
    "    lidando com o cold start e considerando diversidade básica.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cliente_id = customer_encoder.transform([nome_cliente])[0]\n",
    "    except ValueError:\n",
    "        # Cliente não encontrado (Cold Start)\n",
    "        cliente_id = -1\n",
    "\n",
    "    if cliente_id == -1:\n",
    "        # Estratégia de Cold Start: Produtos mais vendidos por categoria\n",
    "        # Calcula as vendas por categoria\n",
    "        sales_by_category = df.groupby(['Category', 'Product ID'])['Sales'].sum().reset_index()\n",
    "\n",
    "        # Para cada categoria, obtém os top-k produtos mais vendidos\n",
    "        recommended_products = pd.DataFrame()\n",
    "        for category in df['Category'].unique():\n",
    "            top_products = sales_by_category[sales_by_category['Category'] == category].nlargest(top_k, 'Sales')\n",
    "            recommended_products = pd.concat([recommended_products, top_products])\n",
    "\n",
    "        # Decodifica os IDs dos produtos\n",
    "        recommended_product_ids = recommended_products['Product ID'].unique()\n",
    "        recommended_products_names = product_encoder.inverse_transform(recommended_product_ids)\n",
    "\n",
    "        recommended_df = pd.DataFrame({'Product ID': recommended_products_names})\n",
    "        recommended_df = recommended_df.merge(df[['Product ID', 'Product Name', 'Category', 'Sub-Category']], on='Product ID', how='left').drop_duplicates()\n",
    "\n",
    "    else:\n",
    "        # Cliente conhecido: Prever pontuações\n",
    "        num_products = len(product_encoder.classes_)\n",
    "        product_ids = np.arange(num_products)\n",
    "        customer_ids = np.full((num_products,), cliente_id)\n",
    "        category_ids = np.zeros(num_products)  # Usando 0 como placeholder\n",
    "        subcategory_ids = np.zeros(num_products)  # Usando 0 como placeholder\n",
    "\n",
    "\n",
    "        product_scores = model.predict([customer_ids.reshape(-1, 1), product_ids.reshape(-1, 1), category_ids.reshape(-1, 1), subcategory_ids.reshape(-1, 1)], verbose=0).flatten()\n",
    "\n",
    "        # Obter os top-k produtos recomendados com diversidade básica\n",
    "        # (Seleciona um pouco de cada categoria, se possível)\n",
    "        top_product_indices = np.argsort(product_scores)[::-1]\n",
    "        recommended_products = []\n",
    "        categories_seen = set()\n",
    "        \n",
    "        i = 0\n",
    "        while len(recommended_products) < top_k and i < len(top_product_indices):\n",
    "            product_idx = top_product_indices[i]\n",
    "            product_id_encoded = product_encoder.classes_[product_idx]\n",
    "            product_category = df[df['Product ID Enc'] == product_idx]['Category'].iloc[0] #Pegar a categoria do produto original\n",
    "            \n",
    "            if product_category not in categories_seen:\n",
    "                recommended_products.append(product_id_encoded)\n",
    "                categories_seen.add(product_category)\n",
    "            i += 1\n",
    "\n",
    "        # Se não conseguiu diversificar totalmente, preenche com os melhores restantes\n",
    "        if len(recommended_products) < top_k:\n",
    "            remaining_products = product_encoder.inverse_transform(top_product_indices[:top_k - len(recommended_products)])\n",
    "            recommended_products.extend(remaining_products)\n",
    "            recommended_products = recommended_products[:top_k]  # Garante que não ultrapassa top_k\n",
    "\n",
    "        recommended_df = pd.DataFrame({'Product ID': recommended_products})\n",
    "        recommended_df = recommended_df.merge(df[['Product ID', 'Product Name', 'Category', 'Sub-Category']], on='Product ID', how='left').drop_duplicates()\n",
    "\n",
    "    recommended_df.insert(0, 'Ranking', range(1, len(recommended_df) + 1))\n",
    "    return recommended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86089bfd-d581-498f-98a9-61973f346ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Treinamento e Avaliação ---\n",
    "\n",
    "def train_and_evaluate(df, customer_encoder, product_encoder, category_encoder, subcategory_encoder, epochs=20, batch_size=32):\n",
    "    \"\"\"Treina e avalia o modelo.\"\"\"\n",
    "\n",
    "    (customer_ids_train, customer_ids_test,\n",
    "     product_ids_train, product_ids_test,\n",
    "     category_ids_train, category_ids_test,\n",
    "     subcategory_ids_train, subcategory_ids_test,\n",
    "     sales_train, sales_test) = split_data(df)\n",
    "\n",
    "    num_customers = len(customer_encoder.classes_)\n",
    "    num_products = len(product_encoder.classes_)\n",
    "    num_categories = len(category_encoder.classes_)\n",
    "    num_subcategories = len(subcategory_encoder.classes_)\n",
    "    embedding_dim = 16  \n",
    "\n",
    "    model = create_model(num_customers, num_products, num_categories, num_subcategories, embedding_dim)\n",
    "\n",
    "    # Callbacks (TensorBoard)\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    model.fit(\n",
    "        [customer_ids_train.reshape(-1, 1), product_ids_train.reshape(-1, 1), category_ids_train.reshape(-1, 1), subcategory_ids_train.reshape(-1, 1)],\n",
    "        sales_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Avaliação\n",
    "    mse_train, precision_train, recall_train, f1_train = evaluate_model(\n",
    "        model, customer_ids_train, product_ids_train, category_ids_train, subcategory_ids_train, sales_train\n",
    "    )\n",
    "    mse_test, precision_test, recall_test, f1_test = evaluate_model(\n",
    "        model, customer_ids_test, product_ids_test, category_ids_test, subcategory_ids_test, sales_test\n",
    "    )\n",
    "\n",
    "    print(f\"Treino: MSE = {mse_train:.4f}, Precision@{7} = {precision_train:.4f}, Recall@{7} = {recall_train:.4f}, F1@{7} = {f1_train:.4f}\")\n",
    "    print(f\"Teste: MSE = {mse_test:.4f}, Precision@{7} = {precision_test:.4f}, Recall@{7} = {recall_test:.4f}, F1@{7} = {f1_test:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56964204-e59b-439f-903b-5e86f51bcdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caminho completo: C:\\Users\\flavi\\Documents\\GitHub\\Projeto_7_Sistema_de_Recomendacao\\dados\\dados_tratados.parquet\n",
      "Arquivo existe? True\n"
     ]
    }
   ],
   "source": [
    "# Sobe para a raiz do projeto (assumindo que o notebook está em /notebooks)\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))  # Adiciona a raiz ao Python path\n",
    "\n",
    "# Importação correta\n",
    "from src.config.paths import DADOS_TRATADOS\n",
    "\n",
    "# Verificação\n",
    "print(f\"Caminho completo: {DADOS_TRATADOS}\")\n",
    "print(f\"Arquivo existe? {DADOS_TRATADOS.exists()}\")\n",
    "\n",
    "# Uso no código\n",
    "file_path = DADOS_TRATADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac889f1-d8f9-450e-b0d9-accb762c3add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 4.5799e-04  \n",
      "Epoch 2/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.1083e-04  \n",
      "Epoch 3/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5649e-04    \n",
      "Epoch 4/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0751e-04  \n",
      "Epoch 5/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0802e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.2405e-05    \n",
      "Epoch 7/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.3471e-05  \n",
      "Epoch 8/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.4858e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9822e-05  \n",
      "Epoch 10/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.6566e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.6251e-05  \n",
      "Epoch 12/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.7719e-05  \n",
      "Epoch 13/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.8229e-05  \n",
      "Epoch 14/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.6849e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3257e-05    \n",
      "Epoch 16/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.8150e-05   \n",
      "Epoch 17/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.9853e-05  \n",
      "Epoch 18/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.7706e-05  \n",
      "Epoch 19/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.3984e-05  \n",
      "Epoch 20/20\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.4641e-05  \n",
      "Treino: MSE = 0.0000, Precision@7 = 0.1429, Recall@7 = 1.0000, F1@7 = 0.2500\n",
      "Teste: MSE = 0.0009, Precision@7 = 0.1429, Recall@7 = 1.0000, F1@7 = 0.2500\n"
     ]
    }
   ],
   "source": [
    "# --- Main ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Caminho para os dados \n",
    "    file_path = DADOS_TRATADOS\n",
    "\n",
    "    # Carregar e pré-processar os dados\n",
    "    df, customer_encoder, product_encoder, category_encoder, subcategory_encoder = load_and_preprocess_data(file_path)\n",
    "\n",
    "    # Treinar e avaliar o modelo\n",
    "    trained_model = train_and_evaluate(df, customer_encoder, product_encoder, category_encoder, subcategory_encoder)\n",
    "\n",
    "    # Gerar recomendações para um cliente específico\n",
    "    cliente_para_recomendar = \"Irene Maddox\"  \n",
    "    recomendacoes = recomendar_produtos(cliente_para_recomendar, df, trained_model, customer_encoder, product_encoder, category_encoder, subcategory_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e93df36f-2589-4341-aceb-d734d0be5df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recomendações para Irene Maddox:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>OFF-SU-10002881</td>\n",
       "      <td>Martin Yale Chadless Opener Electric Letter Op...</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>FUR-CH-10001215</td>\n",
       "      <td>Global Troy Executive Leather Low-Back Tilter</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>TEC-CO-10004722</td>\n",
       "      <td>Canon imageCLASS 2200 Advanced Copier</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>OFF-AR-10002671</td>\n",
       "      <td>Hunt BOSTON Model 1606 High-Volume Electric Pe...</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5</td>\n",
       "      <td>OFF-AP-10002945</td>\n",
       "      <td>Honeywell Enviracaire Portable HEPA Air Cleane...</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Appliances</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ranking       Product ID  \\\n",
       "0         1  OFF-SU-10002881   \n",
       "6         2  FUR-CH-10001215   \n",
       "15        3  TEC-CO-10004722   \n",
       "26        4  OFF-AR-10002671   \n",
       "36        5  OFF-AP-10002945   \n",
       "\n",
       "                                         Product Name         Category  \\\n",
       "0   Martin Yale Chadless Opener Electric Letter Op...  Office Supplies   \n",
       "6       Global Troy Executive Leather Low-Back Tilter        Furniture   \n",
       "15              Canon imageCLASS 2200 Advanced Copier       Technology   \n",
       "26  Hunt BOSTON Model 1606 High-Volume Electric Pe...  Office Supplies   \n",
       "36  Honeywell Enviracaire Portable HEPA Air Cleane...  Office Supplies   \n",
       "\n",
       "   Sub-Category  \n",
       "0      Supplies  \n",
       "6        Chairs  \n",
       "15      Copiers  \n",
       "26          Art  \n",
       "36   Appliances  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nRecomendações para {cliente_para_recomendar}:\\n\")\n",
    "recomendacoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c527d9e-fecc-441a-83e5-863549edc346",
   "metadata": {},
   "source": [
    "### Salvando o modelo para persistência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e13498fa-1c20-4129-b70c-33e03ad5cca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo salvo com sucesso em:\n",
      "C:\\Users\\flavi\\Documents\\GitHub\\Projeto_7_Sistema_de_Recomendacao\\models\\best_model_recomendacao.keras\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, filename: str, overwrite: bool = False) -> Path:\n",
    "    \"\"\"\n",
    "    Salva o modelo na pasta models com tratamento de erros.\n",
    "    \n",
    "    Parâmetros:\n",
    "        model: Modelo treinado (Keras/TensorFlow)\n",
    "        filename: Nome do arquivo (com extensão .keras ou .h5)\n",
    "        overwrite: Se True, sobrescreve arquivos existentes\n",
    "        \n",
    "    Retorna:\n",
    "        Caminho completo do modelo salvo\n",
    "    \"\"\"\n",
    "    model_path = MODELS_DIR / filename\n",
    "    \n",
    "    if model_path.exists() and not overwrite:\n",
    "        raise FileExistsError(f\"Arquivo {model_path} já existe. Use overwrite=True para substituir.\")\n",
    "    \n",
    "    # Garante que a extensão é válida\n",
    "    if not filename.endswith(('.keras', '.h5')):\n",
    "        raise ValueError(\"Extensão inválida. Use '.keras' ou '.h5'\")\n",
    "    \n",
    "    model.save(model_path)\n",
    "    \n",
    "    # Verificação pós-salvamento\n",
    "    if not model_path.exists():\n",
    "        raise RuntimeError(f\"Falha ao salvar o modelo em {model_path}\")\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "# Uso:\n",
    "try:\n",
    "    saved_path = save_model(trained_model, \"best_model_recomendacao.keras\")\n",
    "    print(f\"✅ Modelo salvo com sucesso em:\\n{saved_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao salvar modelo: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (TensorFlow)",
   "language": "python",
   "name": "meu_env_python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
